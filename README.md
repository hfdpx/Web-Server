# A C++ High Performance Web Server  

## 介绍 
本项目为C++11编写的轻量级高性能的Web服务器，解析了get，head请求，可处理静态资源，支持HTTP长连接，支持异步日志，能记录服务器运行状态，支持Cache机制，能提高服务器的响应速度  

测试页:[http://47.93.39.164:8080/](http://47.93.39.164:8080/)

## 环境  
* OS:CentOS 7.2
* Complier:GCC 4.8.5

## 使用
     make
    ./web_final

## 技术要点
* 使用Epoll边沿触发的IO多路复用技术，使用Reactor模式构建程序框架

* 使用多线程充分利用多核CPU，并使用线程池避免线程频繁创建销毁的开销

* 使用生产者消费者模型处理任务

* 使用Cache机制，将用户群体经常访问的页面放入Cache中，减少磁盘读写次数和保证响应速度

* 使用LRU算法淘汰Cahce中的文件，保证页面命中率  

* 使用Mutex互斥锁和condition条件变量保证线程安全

* 使用Segtion lock分段锁机制提高多线程运行效率

* 使用状态机机制解析Http请求，保证用户的所有合法请求都能被处理

* 为减少内存泄漏的可能，采用了智能指针

* 构建了一个高效的buffer，支持自适应扩容，内存连续，在必要时可利用临时的栈上空间暂存数据

* 构建了一个高效异步日志库，能记录服务器运行时的状态，一秒钟可写百万条日志到磁盘！  

* 使用基于小根堆的定时器关闭超时请求，避免资源被长期占用

## 版本历史

 
### 版本1：单进程+BIO 的web server!  
  * 使用包裹函数处理错误  
  
  当没有连接到来的时候,该进程会阻塞在Accept函数上,  
  当有多个连接到来的时候，只能处理一个连接，其他连接进入阻塞队列等待，  
  
  当然,这里的connfd也是阻塞版本的！也就是说,在对connfd执行write,read等函数的时候也会阻塞.  
  比如一个连接要先reda资源然后write资源，如果要reda的时候服务器没有准备好，那么系统会阻塞处理该连接的进程，等待要read的资源准备好  
  
  这样一来,这个版本的server效率会非常低下！
  
  
  
### 版本2：多进程+BIO 的web server!

  * 比单进程+BIO版本的server效率有所提高  

  当没有连接到来的时候,该进程会阻塞在Accept函数上,  
  当有多个连接到来的时候，为每一个连接fork一个进程  

  当然,这里的connfd也是阻塞版本的！也就是说,在对connfd执行write,read等函数的时候也会阻塞.  
  比如一个连接要先reda资源然后write资源，如果要reda的时候服务器没有准备好，那么系统会阻塞处理该连接的进程，等待要read的资源准备好  

  因为是BIO的，每个进程可能因为连接没有请求到相应的资源而阻塞

  这样一来，多进程+BIO版本的server效率仍然不是很高！

###### 解决的bug:  
  * 解决了僵死进程的问题

### 版本3:多线程+BIO 的web server!
  
  * 和多进程+BIO版本的web server各有千秋
  * 其效率取决于server的业务和物理机器的硬件资源  
###### 解决的问题:  
  *  连接频繁的情况下，还没有来得及传递给新线程的套接字描述符被新到来的连接覆盖导致某些客户的连接无法响应的问题
  *  解决了 解决上述问题采取的方案 导致的线程运行中内存泄漏的问题
  *  实现了资源的自动回收
###### 总结:
跟多进程+BIO的版本相差不大,都是为每个新的连接建立一个进程或者线程,下一版本可以考虑采用线程池降低线程创建和销毁的开销


#### ps:关于多进程和多线程的对比
###### “鱼和熊掌不可兼得”  
  * 数据的共享与同步：
    * 多进程数据共享复杂，需要用IPC，数据是分开的，同步也简单
    * 多线程共享进程数据，所以数据共享简单，但同步复杂
  * 内存和CPU占比：
    * 多进程占用内存多，切换复杂，CPU利用率低
    * 多线程占用内存少，切换简单，CPU利用率高  
  * 创建和销毁：
    * 进程的创建，销毁和切换复杂，速度慢
    * 线程的创建，销毁和切换简单，速度快
  * 可靠性：  
    * 进程间不会互相影响，一个进程的崩溃不会影响其他进程
    * 一个线程挂掉将导致整个进程挂掉
  * 分布式：
    * 适用于多核，多机分布式，如果一台机器不够，扩展到多台机器简单
    * 只使用于多核分布式
  * 编程和调试：  
    * 多进程编程和调试简单
    * 多线程编程和调试复杂
###### 写Web Server个人倾向于使用多线程  

### 版本4:线程池+BIO 的web server  
合理的利用线程池能带来三个好处:    

* 1.降低资源消耗,不用频繁的创建和消耗线程    
* 2.提高响应速度，任务可以不需要等待线程建立就可以立即执行，这点对web server来说至关重要！   
* 3.提高线程的可管理性，毕竟来一个连接创建一个线程这种做法导致我们无法进行统一的管理和调配  

###### 采取的措施:  
* 1.参照陈硕大神的muduo库，实现了一个条件变量condition类，用来实现线程之间的协作  
* 2.同样也是参照muduo库，实现了一个互斥锁Mutex类，利用RAll机制，实现了自动加锁和解锁  
* 3.线程池中任务队列采用任务数组和头尾指针滚动的形式实现，线程全部预先创建
* 4.其他做法跟多线程+BIO版本的web server类似  
###### 解决的bug：
* 虚假唤醒问题  
###### 存在的问题:
* 任务太多可能导致任务队列中没有完成的任务被覆盖  
* 线程全部预先创建没有必要  
###### 总结:  
线程池的实现简陋了点，下一版本目标是实现一个完整的线程池，解决上面存在的问题

### 版本5:完整的线程池+BIO 的web server  
* 任务队列采用链表+头尾双指针实现
###### 线程池原理：
* 线程池有一个属性为线程池允许的最大线程数
* 任务到来了则将任务插入到任务队列尾部
* 在任务队列不空的情况下，线程池中存在空闲线程则将队首任务交给该空闲线程，不存在空闲线程的话，如果当前线程数量小于线程池允许的最大线程数则创建新线程执行队首任务，否则将等待其他线程空闲
##### 存在的问题：
* 因为是BIO，所以效率仍然达不到高性能web server的要求！
##### 总结：  
* 解决了版本4存在的问题，整个线程池非常完整，封装得也还可以！下一版本的目标是实现非阻塞IO  
### 版本6:单线程 + IO多路复用(epoll LT) + Reactor模式 的web server  
* 采用epoll，使用LT模式实现IO多路复用

* 将http处理代码封装成了Httphandle类，配备读写缓冲区，连接描述符，发送的文件信息，以及读写相关指针等信息  

* 采用Cache机制，将用户群体经常访问的一些页面放入Cache中，提高响应速度  

* Cache机制中采用了智能指针，方便缓存资源的释放  

* 采用OS的LRU最近最久未使用页面置换算法实现Cache中文件的置换，以便更少的产生"缺页"现象  

* 采用状态机机制实现页面请求的处理,便于处理请求  

##### 不足:  
  * LT模式可以改为效率更高的ET模式
  * 可以采用多线程进行处理

### 版本7:单线程 + IO多路复用(epoll ET) +Reactor模式 的web server  
* 采用比LT模式效率更高的ET模式，目的是减少事件被触发的次数  
* 其他同上版本  
### 版本8:线程池+ IO多路复用(epoll ET) + Reactor模式 的web server  
* Reactor模式

* 实现了一个线程池和生产者消费者模型  

* 实现了一个条件变量condition类，用来实现线程之间的协作  

* 实现了一个互斥锁Mutex类，利用RAll机制，实现了自动加锁和解锁  

* 任务队列的锁采用分段锁机制，头部一个锁，尾部一个锁，插入任务和获得任务可以同时进行，统计任务数量则需要获得两把锁  

* 采用两个条件变量代表任务队列可读或可写，降低了锁的操作次数

* 封装了一个日志函数

* 使用了右值引用和move语义，提高了效率

* 使用了cpp的新特性，使用boost::funciton和boost::bind代替template，增加了程序可读性
 
* 解决了Cache中shared_ptr多线程不安全问题  

* 通过EPOLLONESHOT避免了一个连接的数据被多个线程读取的情况  
### 版本9:最后的版本  
* 参考moduo库，实现了一个高效带缓冲的的Buffer类，支持自适应扩容，内存连续，利用了临时的栈上空间暂存数据，带缓冲，不用自己去read()或write某个socket，只用操作TcpConnection的input buffer和output buffer.  
![0_1303014373Vfgb.gif.png](https://i.loli.net/2019/05/10/5cd4ca01a2e97.png)  
![0_1303014373FZQM.gif.png](https://i.loli.net/2019/05/10/5cd4ca33434d5.png)


* 为了更好的处理连接，将处理连接的动作拆分了出来组成了一个HttpRequest类

* 解决了一个连接数据可能被多个线程读的情况

* 完善了状态机机制，服务器现在能有效处理客户端发送不规则数据造成的各种情况

![20161104143716693.png](https://i.loli.net/2019/05/10/5cd4ca484cbe4.png)
![20161104143648365.png](https://i.loli.net/2019/05/10/5cd4ca5640430.png)


## 关于定时器系统的说明
### 基于最小堆的定时器
#### 功能：
断开超时的连接，避免资源被长时间的占用

#### 原理：
利用最小堆的性质：堆顶元素是最小的，来淘汰最先到达超时时间的连接  
堆的操作主要是上滤和下滤操作，用来重新维持最小堆的性质:

* 上滤：将当前结点和父结点比较，如果当前值较小则与父结点进行交换，保证小的往上走，用于结点的插入  
* 下滤：将当前结点和左右孩子结点比较，如果当前结点的孩子比该结点小则将较小的孩子结点和当前结点交换，保证小的往上走，用于结点的删除  
 

#### 小根堆定时器分析:
定时器结点有三个属性:  

* 该定时器生效的绝对时间
* 定时器的回调函数
* 客户端数据(该连接的文件描述符)

许多个的定时器结点构成了一个定时器堆，该堆遵从最小堆的性质，这样最先淘汰的就是最小的时间  

* 堆的结点采用数组存储(根结点：N，左孩子：2N+1,右孩子：2N+2)，不仅节省空间，而且更容易进行插入和删除  

* 堆数组支持自动扩容,自动扩容一倍，另外申请一个原数组一倍大小的空间，逐个迁移过去，最后销毁原数组，一开始就把数组开得足够大，并且过期结点会被删除，所以扩容操作进行的次数很少，不会影响定时器的性能

* 在连接还没有超时的时候，如果连接就关闭了，那么需要同时删除堆中该连接的定时器，这里采用延迟销毁的方式：将该定时器的回调函数置NULL，可以节省删除定时器的开销，在该定时器超时之后才会在心跳函数中被真正删除

* 心跳函数能删除所有过期结点：堆顶元素过期则删除堆顶元素然后将堆底元素放到堆顶，然后进行下滤重新恢复小根堆的性质

* 连接刚到的时候不会将该连接的定时器加入堆中，只会对该连接的定时器进行初始化，当改连接被处理过一次之后才会将连接的定时器加入到堆中，这样可以使得堆中的元素尽可能的少，因为连接被处理过一次之后大部分都完成了所有任务然后关闭了连接，这种被处理得很快的连接没有必要被执行定时操作

#### 定时器时间复杂度分析：  
评判定时器性能的四个要素：  

* StartTimer：注册一个定时器结点到总定时器中  

* StopTimer：找到超时的定时器  

* PerTickBookkeeping：在一个tick内，定时器系统需要执行的动作，它的主要行为就是检测定时器系统中是否有定时器到期

* ExpiryProcessing：在定时器到期之后，执行定时器的回调函数  

对我们基于小根堆的定时器而言：  

* StartTimer：就是往堆中添加定时器结点：O(lg n)

* StopTimer:就是找堆顶元素：O(1)  

* PerTickBookkeeping:就是检测堆中是否有超时的，直接检测堆顶即可：O(1)

* ExpiryProcessing:执行定时器的回调函数:O(1)

综上所述，我们可以得出一个结论：基于小根堆的定时器其性能还是不错的！  
ps：后面会采用时间轮的方式实现定时器，这样插入定时器结点为O(1),性能进一步提升

## 环形异步日志系统说明

### **简介**
环形异步日志系统g是一个适用于C++的异步日志， 其特点是**效率高（每秒支持125+万日志写入）、易拓展**，尤其适用于**频繁写日志的场景**

**效率高**：建立日志缓冲区、优化UTC日志时间生成策略(**比没有优化前性能提升3倍**)

**易拓展**：基于双向循环链表构建日志缓冲区，其中每个节点是一个小的日志缓冲区

而：
传统日志：直接走磁盘
而“基于队列的异步日志”：每写一条日志就需要通知一次后台线程，在频繁写日志的场景下通知过多，且队列内存不易拓展

### **源代码** :  
rlog.cc rlog.h(代码总计不到600行)
### **工作原理**
#### **数据结构**
环形异步日志系统的缓冲区是若干个`cell_buffer`以双向、循环的链表组成
`cell_buffer`是简单的一段缓冲区，日志追加于此，带状态：
- `FREE`：表示还有空间可追加日志
- `FULL`：表示暂时无法追加日志，正在、或即将被持久化到磁盘；

环形异步日志系统有两个指针：
- `Producer Ptr`：生产者产生的日志向这个指针指向的`cell_buffer`里追加，写满后指针向前移动，指向下一个`cell_buffer`；`Producer Ptr`永远表示当前日志写入哪个`cell_buffer`，**被多个生产者线程共同持有**
- `Consumer Ptr`：消费者把这个指针指向的`cell_buffer`里的日志持久化到磁盘，完成后执行向前移动，指向下一个`cell_buffer`；`Consumer Ptr`永远表示哪个`cell_buffer`正要被持久化，仅被**一个后台消费者线程持有**

![Alt text](pictures(log)/mainstructor.png)

起始时刻，每个`cell_buffer`状态均为`FREE`
`Producer Ptr`与`Consumer Ptr`指向同一个`cell_buffer`

整个Ring Log被一个互斥锁`mutex`保护

#### **大致原理**

**消费者**

后台线程（消费者）forever loop：
1. 上锁，检查当前`Consumer Ptr`：
 - 如果对应`cell_buffer`状态为`FULL`，释放锁，去*STEP 4*；
 - 否则，以1秒超时时间等待条件变量`cond`；
2. 再次检查当前`Consumer Ptr`：
 - 若`cell_buffer`状态为`FULL`，释放锁，去*STEP 4*；
 - 否则，如果`cell_buffer`无内容，则释放锁，回到*STEP 1*；
 - 如果`cell_buffer`有内容，将其标记为`FULL`，同时`Producer Ptr`前进一位；
3. 释放锁
4. 持久化`cell_buffer`
5. 重新上锁，将`cell_buffer`状态标记为`FREE`，并清空其内容；`Consumer Ptr`前进一位；
6. 释放锁

**生产者**

1. 上锁，检查当前`Producer Ptr`对应`cell_buffer`状态：
如果`cell_buffer`状态为`FREE`，且生剩余空间足以写入本次日志，则追加日志到`cell_buffer`，去*STEP X*；
2. 如果`cell_buffer`状态为`FREE`但是剩余空间不足了，标记其状态为`FULL`，然后进一步探测下一位的`next_cell_buffer`：
 - 如果`next_cell_buffer`状态为`FREE`，`Producer Ptr`前进一位，去*STEP X*；
 - 如果`next_cell_buffer`状态为`FULL`，说明`Consumer Ptr` = `next_cell_buffer`，Ring Log缓冲区使用完了；则我们继续申请一个`new_cell_buffer`，将其插入到`cell_buffer`与`next_cell_buffer`之间，并使得`Producer Ptr`指向此`new_cell_buffer`，去*STEP X*；
3. 如果`cell_buffer`状态为`FULL`，说明此时`Consumer Ptr` = `cell_buffer`，丢弃日志；
4. 释放锁，如果本线程将`cell_buffer`状态改为`FULL`则通知条件变量`cond`

>在大量日志产生的场景下，Ring Log有一定的内存拓展能力；实际使用中，为防止Ring Log缓冲区无限拓展，会限制内存总大小，当超过此内存限制时不再申请新`cell_buffer`而是丢弃日志

#### **图解各场景**
初始时候，`Consumer Ptr`与`Producer Ptr`均指向同一个空闲`cell_buffer1`

![Alt text](pictures(log)/init.png)

然后生产者在1s内写满了`cell_buffer1`，`Producer Ptr`前进，通知后台消费者线程持久化

![Alt text](pictures(log)/step1.png)

消费者持久化完成，重置`cell_buffer1`，`Consumer Ptr`前进一位，发现指向的`cell_buffer2`未满，等待

![Alt text](pictures(log)/step1.5.png)

超过一秒后`cell_buffer2`虽有日志，但依然未满：消费者将此`cell_buffer2`标记为`FULL`强行持久化，并将`Producer Ptr`前进一位到`cell_buffer3`

![Alt text](pictures(log)/step2.png)

消费者在`cell_buffer2`的持久化上延迟过大，结果生产者都写满`cell_buffer3\4\5\6`，已经正在写`cell_buffer1`了

![Alt text](pictures(log)/step3.png)

生产者写满写`cell_buffer1`，发现下一位`cell_buffer2`是`FULL`，则拓展换冲区，新增`new_cell_buffer`

![Alt text](pictures(log)/step4.png)



### **UTC时间优化**

每条日志往往都需要UTC时间：`yyyy-mm-dd hh:mm:ss`（PS：Ring Log提供了毫秒级别的精度）
Linux系统下本地UTC时间的获取需要调用`localtime`函数获取年月日时分秒
在`localtime`调用次数较少时不会出现什么性能问题，但是写日志是一个大批量的工作，如果每条日志都调用`localtime`获取UTC时间，性能无法接受
>在实际测试中，对于1亿条100字节日志的写入，未优化`locatime`函数时 RingLog写内存耗时`245.41s`，仅比传统日志写磁盘耗时`292.58s`快将近一分钟；
>而在优化`locatime`函数后，RingLog写内存耗时`79.39s`，速度好几倍提升

#### **策略**
为了减少对`localtime`的调用，使用以下策略

环形异步日志系统使用变量`_sys_acc_sec`记录写上一条日志时，系统经过的秒数（从1970年起算）、使用变量`_sys_acc_min`记录写上一条日志时，系统经过的分钟数，并缓存写上一条日志时的年月日时分秒year、mon、day、hour、min、sec，并缓存UTC日志格式字符串

每当准备写一条日志：
1. 调用`gettimeofday`获取系统经过的秒`tv.tv_sec`，与`_sys_acc_sec`比较；
2. 如果`tv.tv_sec` 与 `_sys_acc_sec`相等，说明此日志与上一条日志在同一秒内产生，故**年月日时分秒**是一样的，直接使用缓存即可；
3. 否则，说明此日志与上一条日志不在同一秒内产生，继续检查：`tv.tv_sec/60`即系统经过的分钟数与`_sys_acc_min`比较；
4. 如果`tv.tv_sec/60`与`_sys_acc_min`相等，说明此日志与上一条日志在同一分钟内产生，故**年月日时分**是一样的，年月日时分 使用缓存即可，而秒`sec` = `tv.tv_sec%60`，更新缓存的秒sec，重组UTC日志格式字符串的秒部分；
5. 否则，说明此日志与上一条日志不在同一分钟内产生，调用`localtime`**重新获取UTC时间**，并更新缓存的年月日时分秒，重组UTC日志格式字符串

>**小结**：如此一来，`localtime`一分钟才会调用一次，频繁写日志几乎不会有性能损耗

### **性能测试**

对比传统同步日志、与环形异步日志系统的效率（为了方便，传统同步日志以sync log表示）

#### **1. 单线程连续写1亿条日志的效率**
分别使用`Sync log`与`环形异步日志系统`写1亿条日志（每条日志长度为100字节）测试调用总耗时，测5次，结果如下：

| 方式 |  第1次 | 第2次 | 第3次 | 第4次 | 第5次 | 平均 | 速度/s  |
|:----: |:----:  |:----: |:----: |:----:  |:----: |:----:|:----:|
| Sync Log |290.134s|298.466s|287.727s|285.087s|301.499s|292.583s|34.18万/s|
| Ring Log |79.816s| 78.694s|79.489s|79.731s|79.220s|79.39s|125.96万/s|

>单线程运行下，`环形异步日志系统`写日志效率是传统同步日志的近`3.7`倍，可以达到**每秒127万条**长为*100字节*的日志的写入

#### **2、多线程各写1千万条日志的效率**
分别使用`Sync log`与`环形异步日志系统`开5个线程各写1千万条日志（每条日志长度为100字节）测试调用总耗时，测5次，结果如下：

| 方式 |  第1次 | 第2次 | 第3次 | 第4次 | 第5次 | 平均 | 速度/s  |
|:----: |:----:  |:----: |:----: |:----:  |:----: |:----:|:----:|
| Sync Log |141.727s|144.720s|142.653s|138.304|143.818s|142.24s|35.15万/s|
| Ring Log |36.896s|37.011s|38.524s|37.197s|38.034s|37.532s|133.22万/s|

>多线程（5线程）运行下，`环形异步日志系统`写日志效率是传统同步日志的近`3.8`倍，可以达到**每秒135.5万条**长为*100字节*的日志的写入



#### **2. 对server QPS的影响**
现有一个Reactor模式实现的echo Server，其纯净的QPS大致为`19.32万/s`
现在分别使用`Sync Log`、`环形异步日志系统`来测试：echo Server在每收到一个数据就调用一次日志打印下的QPS表现

对于两种方式，分别采集12次实时QPS，统计后大致结果如下：

| 方式 |  最低QPS | 最高QPS | 平均QPS | QPS损失比 |
|:----: |:----:  |:----: |:----: | :----:|
| `Sync Log` |96891次|130068次|114251次| 40.89%|
| `环形异步日志系统` |154979次 |178697次 |167198次|13.46% |

>传统同步日志`sync log`使得echo Server QPS从19.32w万/s降低至`11.42万/s`，损失了`40.89%`
>`环形异步日志系统`使得echo Server QPS从19.32w万/s降低至`16.72万/s`，损失了`13.46%`

### **USAGE**


>LOG_INIT("logdir", "myapp");
>
>LOG_ERROR("my name is %s, my number is %d", "leechanx", 3);

最后会在目录logdir下生成myapp.yyyy-mm-dd.pid.log.[n]文件名的日志

日志格式为：
>[ERROR][yyyy-mm-dd hh:mm:ss.ms][pid]code.cc:line_no(function_name): my name is leechanx, my number is 3


### **TODO**
- 程序正常退出、异常退出，此时在buffer中缓存的日志会丢失（通过把堆内存替换为共享内存来解决）
- 第N天23:59:59秒产生的日志有时会被刷写到第N+1天的日志文件中


## 服务器性能测试

### 测试环境
* OS：Ubuntu 14.04
* 内存：8G
* CPU：I7-4720HQ

### 测试方法
* 理想的测试环境是两台计算机，带宽无限，现在的网卡虽然都是千兆网卡，但是普通家用的网线都是5类双绞线，最高100Mbps,在linux下用ethtool可以看到网卡的速度被限制为100Mbsp，无法更改为更高的，经测试很容易跑满带宽，因此退而选择本地环境。
* 使用工具Webbench，开启1000客户端进程，时间为60s
* 分别测试短连接和长连接的情况
* 关闭所有的输出及Log
* 为避免磁盘IO对测试结果的影响，测试响应为内存中的"Hello World"字符加上必要的HTTP头
* 我的最终版本中很多方面借鉴了muduo的思路，muduo中也提供了一个简单的HTTP echo测试，因此我将与muduo进行一个小小的对比，我修改了muduo测试的代码，使其echo相同的内容，关闭muduo的所有输出及Log
* 线程池开启4线程
* 因为发送的内容很少，为避免发送可能的延迟，关闭Nagle算法


### 测试结果及分析
测试截图放在最后  

| 服务器 | 短连接QPS | 长连接QPS | 
| - | :-: | -: | 
| WebServer | 126798| 335338 | 
| Muduo | 88430 | 358302 | 

* 首先很明显的一点是长链接能处理的请求数是短连接的三四倍，因为没有了连接建立和断开的开销，不需要频繁accept和shutdown\close等系统调用，也不需要频繁建立和销毁对应的结构体。
* 我的服务器在最后的版本中，没有改进输入输出Buffer，用了效率低下的string，muduo用的是设计良好的vector<char>，我将在后续改进这一点。这也造成了在长连接的情况下，我的server逊于muduo。虽说边沿触发效率高一点，但是还是比不过在Buffer上性能的优化的。
* 短链接的情况下，我的服务器要超过Muduo很多。原因在于：Muduo采用水平触发方式(Linux下用epoll)，并且做法是每次Acceptor只accept一次就返回，面对突然的并发量，必然会因为频繁的epoll_wait耽误大量的时间，而我的做法是用while包裹accept，一直accept到不能再accept。当然，如果同时连接的请求很少，陈硕在书中也提到过，假如一次只有一个连接，那么我的方式就会多一次accpet才能跳出循环，但是这样的代价似乎微不足道啊，换来的效率却高了不少。
* 空闲时，Server几乎不占CPU，短连接时，各线程的CPU负载比较均衡，长连接时，主线程负载0，线程池的线程负载接近100%，因为没有新的连接需要处理。各种情况均正常。
* 没有严格的考证，测试时发现，HTTP的header解析的结果用map比用unordered_map快，网上的博客里有很多人做了测试，我在做实验的时候大致也发现了。主要是因为数据量太小，一个HTTP请求头才几个头部字段，建立unordered_map的成本要比map高，数据量小，复杂度根本体现不出来。


## 收获
* 增加了自己的编码能力。
* 加深了对linux系统api的了解
* 学到了更多关于linux服务器的知识。
* 想太多是没用的，先考虑实现，再考虑性能。在写代码前想太多是没有意义的。
* Code review的重要性！就算是自己review自己的代码，都能发现一些显而易见的错误。
* Talk is cheap，show me the code。
